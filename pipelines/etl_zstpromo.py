import os
import re
import shutil
import pandas as pd
from math import ceil
from pathlib import Path
from collections import defaultdict
from ETL_SAP.common.loader import upload_to_sql, upsert_batch
from ETL_SAP.common.config import get_sql_engine
from datetime import datetime
from sqlalchemy.types import VARCHAR, NVARCHAR, DECIMAL, INTEGER, Date, DateTime
from ETL_SAP.sap_scripts.downloader_zstpromo import download_zstpromo
from dotenv import load_dotenv
from ETL_SAP.pipelines.etl_utils import *
from ETL_SAP.pipelines.etl_zmb51 import run_etl_zmb51

load_dotenv()

def run_etl_zstpromo(folder_path):
    BATCH_SIZE = 1
    txt_files = sorted(Path(folder_path).glob("ZSTPROMO_*.txt"))
    n_batches = ceil(len(txt_files) / BATCH_SIZE)
    processed_dir = Path(folder_path, "processed")
    processed_dir.mkdir(exist_ok=True)

    print(f"Total {len(txt_files)} files, {n_batches} batches.")

    for b in range(n_batches):
        batch_files = txt_files[b * BATCH_SIZE : (b + 1) * BATCH_SIZE]
        if not batch_files:
            break

        print(f"ğŸšš è™•ç†æ‰¹æ¬¡ {b+1}/{n_batches}ï¼Œæª”æ¡ˆæ•¸ {len(batch_files)} â€¦")

        dfs = []
        for fp in batch_files:
            df = pd.read_csv(fp, sep="\t", skiprows=2, dtype=str, low_memory=False).iloc[:, 1:]  
            df.columns = df.columns.str.strip()
            df['Bill. Date'] = pd.to_datetime(df['Bill. Date'], format='%m/%d/%Y')
            df.dropna(subset=['Article', 'Payer', 'Bill. Date'], inplace=True)
            dfs.append(df)

        batch_df = pd.concat(dfs, ignore_index=True)

        # æ¬„ä½æ­£å
        batch_df = batch_df.rename(columns={
            "Payer": "Site",
            "Bill.qty":  "Quantity",
            "Bill. Date": "Date",
            "Sales Amou": "Amt",
            "SU": "SUn",
        })

        # æ•¸å­—æ¸…æ´—
        batch_df[["Quantity", "Amt", "Cost"]] = batch_df[["Quantity", "Amt", "Cost"]].apply(fast_numeric)

        groupby_df = batch_df.groupby(['Article', 'Site', 'Date']).agg({
            "Amt": 'sum',
            'Quantity': 'sum',
            'Cost': 'sum',
            'SUn' : 'first'
            }).reset_index()    

        print(f"ğŸšš æ‰¹æ¬¡ {b+1} æ¸…æ´—å¾Œè³‡æ–™ï¼š\n{groupby_df.head(2)}\n"
            f"ğŸšš æ‰¹æ¬¡ {b+1} æ¸…æ´—å¾Œè³‡æ–™ç­†æ•¸ï¼š{len(groupby_df)}\n")
        

        # ä¸Šå‚³è‡³ SQL Server
        print(f"ğŸ”¹ é–‹å§‹ä¸Šå‚³ ZSTPROMO è³‡æ–™åˆ° {os.getenv('SQL_DB')}...")

        engine = get_sql_engine()
        column_types = {
            "Article": NVARCHAR(20),
            "Site":    NVARCHAR(10),
            "Date":    Date(),
            "Amt":     DECIMAL(18,6),
            "Quantity":DECIMAL(18,6),
            "Cost":    DECIMAL(18,6),
            "SUn":     NVARCHAR(10),
        }

        upsert_batch(
            df=groupby_df,
            target_table=os.getenv("TABLE_ZSTPROMO"),
            unique_keys=["Article", "Site", "Date"],
            column_types=column_types
         )
        
        # upload_to_sql(groupby_df, os.getenv("TABLE_ZSTPROMO"), column_types, if_exists="append")
        print(f"âœ… æ‰¹æ¬¡ {b+1} å·²åŒ¯å…¥ {os.getenv("TABLE_ZSTPROMO")} {len(groupby_df):,} åˆ—\n")

        # ---------- ç§»å‹•åˆ° processed ----------
        for fp in batch_files:
            dest = processed_dir / fp.name
            # è‹¥åŒåæª”å·²å­˜åœ¨å°±åŠ æ™‚é–“æˆ³é¿å…è¦†å¯«
            if dest.exists():
                timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
                dest = processed_dir / f"{fp.stem}_{timestamp}{fp.suffix}"
            shutil.move(fp, dest)
        print(f"æ‰¹æ¬¡ {b+1} æª”æ¡ˆå·²ç§»è‡³ {processed_dir}\n")

    print("ğŸ‰ å…¨éƒ¨æ‰¹æ¬¡è™•ç†çµæŸ")


if __name__ == "__main__":

    download_zstpromo(os.getenv("DATE_FILE_ZSTPROMO"), os.getenv("EXPORT_DIR_ZSTPROMO"))
    run_etl_zstpromo(os.getenv("EXPORT_DIR_ZSTPROMO")) 

    # run_etl_zmb51(os.getenv("EXPORT_DIR_ZMB51"))
    



